\section{Approach}
\label{sec:approach}

\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/model_architecture.png}
  \caption{The pipeline of our proposed model, AA3D. It takes an RGB image, a depth map and a text instruction as inputs and predicts where to act (an object/part mask and an interaction point) and how to act (a 3D motion vector and motion type).}
  \label{fig:pipeline}
\end{figure*}

We propose \textbf{AA3D}, a multi-modal model that, given an RGB image $\mathbf{I}\!\in\!\mathbb{R}^{3\times H\times W}$, a depth map $\mathbf{D}\!\in\!\mathbb{R}^{1\times H\times W}$, and a free-form text instruction $\mathbf{T}\!=\!\{t_1,\dots,t_L\}$, predicts: (i) an object/part mask $\hat{\mathbf{M}}\!\in\![0,1]^{H\times W}$ that we threshold to get the binary object mask, and an affordance map $\hat{\mathbf{P}}\!\in\![0,1]^{H\times W}$, and (ii) a 3D motion vector $\hat{\mathbf{v}}\!\in\!\mathbb{R}^3$ and motion-type logits $\hat{\mathbf{y}}\!\in\!\mathbb{R}^K$ where K is the number of motion types. At a high level, AA3D (a) encodes RGB, depth, and text, (b) injects geometry by concatenating learned depth features to CLIP visual features, (c) performs text-conditioned decoding via an FPN and a transformer cross-attention decoder, (d) projects to mask and affordance maps with dynamic, text-conditioned kernels, and (e) predicts motion with a conditional VAE.

\subsection{Optional language augmentation head}

When human-written instructions are unavailable, we generate synthetic instructions $\tilde{\mathbf{T}}$ from available supervision. We prompt a vision–language model (\emph{i.e}, Gemini) with: (i) the input image with all candidate interactable objects outlined, (ii) a second image highlighting the target given by $\mathbf{M}_{\text{gt}}$, and (iii) the ground-truth motion type $y_{\text{gt}}$ as text. You can see an example on [Figure]. The VLM returns a concise imperative instruction (\eg, ``push the drawer handle to the left'') that unambiguously refers to the target:
\[
\tilde{\mathbf{T}}=\operatorname{VLM}\!\Big(\ \mathbf{I}\ \text{(outlined)},\ \mathbf{I}\ \text{(masked/cropped by }\mathbf{M}_{\text{gt}}\text{)},\ y_{\text{gt}}\ \Big).
\]
We encode $\tilde{\mathbf{T}}$ with the same CLIP text encoder as for human text, obtaining token features $\{\tilde{\mathbf{s}}_i\}$ and a pooled state $\tilde{\mathbf{s}}$, which replace $(\mathbf{s}_i,\mathbf{s})$ in downstream modules. Because $\tilde{\mathbf{T}}$ is deterministically derived from $(y_{\text{gt}},\mathbf{M}_{\text{gt}})$ and the scene, this head preserves supervision for other tasks while maintaining multi-modal conditioning. At inference, if the user provides text, this head is disabled.

\subsection{Model architecture} \tk{it is not organized. Hard to follow this section. We need an organized figure where each component exists. I recommend to rewrite this section more clearly.}

\textbf{Visual (CLIP).} As we need to localize image features, we cannot use the single dimension embedding output of CLIP. Following CRIS, we extract multi-scale features $\mathbf{v}_2,\mathbf{v}_3,\mathbf{v}_4$ from intermediate layers with shapes $\mathbf{v}_2\!\in\!\mathbb{R}^{C\times H/8\times W/8}$, $\mathbf{v}_3\!\in\!\mathbb{R}^{C\times H/16\times W/16}$, and $\mathbf{v}_4\!\in\!\mathbb{R}^{C\times H/32\times W/32}$.

\vspace{0.5em}


\noindent\textbf{Text (CLIP).} We encode $\mathbf{T}$ to token embeddings $\{\mathbf{s}_1,\dots,\mathbf{s}_L\}$ and a pooled state $\mathbf{s}$ through CLIP.

\vspace{0.5em}


\noindent\textbf{Depth.} CLIP does not support RGBD input. Therefore, we have two choice; either retrain from scratch with RGBD support, or use a separate depth encoder and merge with outputs. We choose the second option that preserves CLIP's RGB pre-training while injecting geometry.
We learn a lightweight depth encoder $f_{\text{depth}}$ (Conv–BN–ReLU blocks) that downsamples $\mathbf{D}$ and produces $\mathbf{F}^{(8)}\!\in\!\mathbb{R}^{C_8\times H/8\times W/8}$ and $\mathbf{F}^{(16)}\!\in\!\mathbb{R}^{C_{16}\times H/16\times W/16}$; the third stage includes a $2\!\times\!2$ max-pool to reach stride 16. 

\subsubsection{Depth-vision fusion}
We concatenate the learned depth features in strides where the dimensions match: $\tilde{\mathbf{v}}_2=[\mathbf{v}_2;\mathbf{F}^{(8)}]$ and $\tilde{\mathbf{v}}_3=[\mathbf{v}_3;\mathbf{F}^{(16)}]$. This design keeps the pre-trained CLIP channels intact, yet allows geometry to flow into the Feature Pyramid Network.

\subsubsection{FPN and cross-modal decoder}
Now, we have to convert these features of different dimensions into a feature I can use for masking and affordance map prediction. To solve this problem, we use a well-known method \emph{i.e.} Feature Pyramid Network.
We construct a text-conditioned FPN over $(\tilde{\mathbf{v}}_2,\tilde{\mathbf{v}}_3,\mathbf{v}_4)$. The text state is projected as $\bar{\mathbf{s}}=W_s\,\mathbf{s}$ and broadcast to the top level. We fuse top-down, iteratively upscaling and fusing, then passing through a convolutional network to produce three maps  with the same height and width of $H/16 \times W/16$. The three $\mathbf{q}_3,\mathbf{q}_4,\mathbf{q}_5$, then concatenate and pass through another convolutinal layer to obtain
\begin{align*}
\mathbf{F}_q\;=\;
&\ 
    \operatorname{Conv}\big(
        [\mathbf{q}_3;\mathbf{q}_4;\mathbf{q}_5]
    \big)
\\
&\in\mathbb{R}^{B\times d\times H/16\times W/16}.
\end{align*}
A transformer decoder refines $\mathbf{F}_q$ through self-attention over visual tokens and cross-attention to text tokens. From this, we get the text-conditioned feature map $\mathbf{F}_q'\in\mathbb{R}^{B\times d\times H/16\times W/16}$.

\subsubsection{Dynamic projection for mask and point}
Now, we have to refine our features with our text prompt. We choose a ViT model, widely used to perform such tasks between text and images with cross attention. We refine $\mathbf{F}_q'$ to stride 4 via upsampling and convolutions to obtain $\mathbf{U}\in\mathbb{R}^{B\times C_v\times H/4\times W/4}$. 
As we are generating masks from text prompts, we use a dynamic convolution to learn the kernels from the text state $\mathbf{s}$. From $\mathbf{s}$, we predict per-sample dynamic kernels for 2 output maps (mask and point). The weights and biases are generated by a linear layer and reshaped into grouped convolution parameters, after which we apply a grouped dynamic convolution:
\begin{align*}
\mathbf{Y} \;=\;&\ \operatorname{GroupConv}\big(\mathbf{U},\ W^{\star},\ b^{\star};\ \text{groups}{=}B\big) \\
&\in\mathbb{R}^{B\times 2\times H/4\times W/4}.
\end{align*}
We interpret the first channel of $\mathbf{Y}$ as mask logits and the second as point-map logits, upsampling both to $H\times W$ for supervision.

\subsubsection{Differentiable interaction point prediction via soft-argmax}
As we plan to use the interaction point coordinates to condition our motion prediction head, we need to make it differentiable. Therefore, given point logits $\mathbf{S}\in\mathbb{R}^{B\times 1\times H_m\times W_m}$, we set $p=\operatorname{softmax}(\operatorname{vec}(\mathbf{S}))$ over spatial locations and compute the expected pixel coordinates. We normalize to $[0,1]^2$ by dividing by $(W_m\!-\!1,\ H_m\!-\!1)$:
\[
\hat{\mathbf{c}}=\Big(\tfrac{\hat{x}}{W_m-1},\ \tfrac{\hat{y}}{H_m-1}\Big).
\]

\subsubsection{Motion VAE}
Given an image and a command like ``open the cabinet'', and a point to interact with, it is obvious that there can be multiple correct motion vectors. You could move it: By pushing it in or pulling it out. pulling straight out or at a slight upward angle. All of these could be valid methods. A deterministic model would be forced to learn the average of all these valid motions. This averaged motion might be mediocre or even physically implausible.


Therefore, as we expect a probability distribution of plausible motions for a given condition, we use a Variational Autoencoder to solve our problem. We pool object features from $\mathbf{F}_q'$ using the predicted mask, and concatenate them with a global visual descriptor (mean of $\mathbf{v}_4$) and the text state to obtain a condition vector $\mathbf{e}$. We further append the point coordinate $\hat{\mathbf{c}}$ to form $\mathbf{u}=[\mathbf{e};\hat{\mathbf{c}}]$. During training, the encoder receives the ground-truth motion $\mathbf{v}_{\text{gt}}$ and produces a Gaussian posterior with parameters $(\boldsymbol{\mu},\boldsymbol{\sigma})$, from which we sample $z=\boldsymbol{\mu}+\boldsymbol{\sigma}\odot\varepsilon$, $\varepsilon\sim\mathcal{N}(0,\mathbf{I})$. The decoder predicts both the motion vector and motion-type logits:
\[
\hat{\mathbf{v}}=W_v\,\operatorname{MLP}_d([z;\mathbf{u}]),\qquad
\hat{\mathbf{y}}=W_{\text{type}}\,\operatorname{MLP}_d([z;\mathbf{u}]).
\]
At inference, we sample $z\sim\mathcal{N}(0,\mathbf{I})$.

\subsection{Loss functions}

\paragraph{Text-to-pixel segmentation loss.} Referring segmentation aligns a single text embedding with many pixel embeddings. Following CRIS, the ``text-to-pixel contrastive'' objective is equivalent to pixel-wise BCE when the predicted probability is $p=\sigma(\mathbf{z}_t\!\cdot\!\mathbf{z}_v^i)$. We therefore supervise with
\[
\mathcal{L}_{\text{mask}}=\operatorname{BCEWithLogits}\!\Big(\hat{\mathbf{M}}_{\text{logits}},\ \mathbf{M}_{\text{gt}}\Big).
\]

\paragraph{Point heatmap loss.} We convert a single ground-truth coordinate $\mathbf{c}_{\text{gt}}$ into a Gaussian heatmap $\mathbf{P}_{\text{gt}}$ at stride 4 for stable gradients and train with
\[
\mathcal{L}_{\text{point-map}}=\operatorname{BCEWithLogits}\!\Big(\hat{\mathbf{P}}_{\text{logits}},\ \mathbf{P}_{\text{gt}}\Big).
\]

\paragraph{Point coordinate loss.} We additionally supervise the soft-argmax coordinate with an $\ell_1$ penalty, $\ \mathcal{L}_{\text{point-coord}}=\|\hat{\mathbf{c}}-\mathbf{c}_{\text{gt}}\|_1$.
\tk{what are the diff between point heatmap and coordinate?}

\paragraph{Motion VAE loss.} Because datasets such as OPDSynth/Real/Multi can contain randomly flipped directions and inconsistent magnitudes, we train the motion head with a \emph{direction-only} objective that ignores sign and scale:
\[
\mathcal{L}_{\text{motion-vec}}=\mathbb{E}\big[\,1-\cos^2(\hat{\mathbf{v}},\,\mathbf{v}_{\text{gt}})\,\big].
\]
When magnitudes are reliable, this can be replaced by $1-\cos(\cdot)$ or an $\ell_2$ term. The KL term is the standard CVAE divergence which makes the encoder learn to encode the data probability space into a normal distribution, so we can use for later sampling.
\[
\mathcal{L}_{\text{KLD}}=D_{\text{KL}}\!\left(\mathcal{N}(\boldsymbol{\mu},\operatorname{diag}(\boldsymbol{\sigma}^2))\ \big\|\ \mathcal{N}(0,\mathbf{I})\right),
\]
and we supervise motion type with cross-entropy: $\ \mathcal{L}_{\text{motion-type}}=\operatorname{CE}(\hat{\mathbf{y}},y_{\text{gt}})$.

\paragraph{Total loss.} We sum all terms with scalar weights:
\begin{align*}
\mathcal{L}_{\text{total}} =\;&\ 
    \lambda_{\text{mask}}\, \mathcal{L}_{\text{mask}}
  + \lambda_{\text{point-map}}\, \mathcal{L}_{\text{point-map}} \\
  &+ \lambda_{\text{point-coord}}\, \mathcal{L}_{\text{point-coord}} \\
  &+ \lambda_{\text{vec}}\, \mathcal{L}_{\text{motion-vec}} \\
  &+ \lambda_{\text{type}}\, \mathcal{L}_{\text{motion-type}} \\
  &+ \lambda_{\text{vae}}\, \mathcal{L}_{\text{KLD}}\,.
\end{align*}

\subsection{Training details}

\paragraph{Implementation Details.}
Our model is built upon a pre-trained CLIP RN50 visual backbone. The depth encoder consists of two sequential Conv-BN-ReLU blocks, producing feature maps with 128 and 256 channels at $1/8$ and $1/16$ resolutions, respectively. These are concatenated with the visual features from CLIP. The FPN module processes input features with channel sizes [512+128, 1024+256, 1024] and outputs features with channel sizes [256, 512, 1024]. The subsequent text-conditioned transformer decoder has 3 layers, 8 attention heads, a feed-forward network dimension of 1024, and a dropout rate of 0.2. The final dynamic projector for mask and point map generation uses a dropout of 0.5. For motion prediction, the conditional VAE head has a latent dimension of 32 and hidden layers of dimension 256.

\paragraph{Loss Configuration.}
The total loss is a weighted sum of individual task losses. For the mask loss $\mathcal{L}_{\text{mask}}$, we use a combination of Dice and BCE loss with equal weights (0.5 each). The point heatmap for $\mathcal{L}_{\text{point-map}}$ is generated from ground-truth coordinates using a Gaussian with $\sigma=8.0$. The motion type classification loss $\mathcal{L}_{\text{motion-type}}$ uses cross-entropy with 0.1 label smoothing. The weights for the total loss function are set as follows: $\lambda_{\text{mask}}=0.5$, $\lambda_{\text{point-map}}=1.0$, $\lambda_{\text{point-coord}}=0.5$, $\lambda_{\text{vec}}=0.5$, $\lambda_{\text{type}}=0.5$, and $\lambda_{\text{vae}}=0.005$ (corresponding to a VAE weight of 0.5 and a KLD beta of 0.01).

\paragraph{Training.}
We train the model for 30 epochs on a single GPU using mixed-precision (FP16). We use the Adam optimizer with a learning rate of $2 \times 10^{-5}$ and a weight decay of $10^{-4}$. The learning rate is decayed by a factor of 0.1 at epochs 25 and 27 using a multi-step scheduler. All input images are resized to $256 \times 256$ pixels, and we use a batch size of 128 for both training and validation. The entire framework is implemented in PyTorch Lightning.
