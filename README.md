# SegAffordance

A multi-modal deep learning framework for **affordance segmentation** and **motion prediction** on articulated objects. The model takes RGB images, depth maps, and text descriptions as input, and predicts:

- **Segmentation mask** of the target object/part
- **Interaction point** (e.g., handle location)
- **Motion direction** (3D axis vector)
- **Motion type** (translation or rotation)
- **Trajectory prediction** (sequence of 3D points)

## ğŸ—ï¸ Architecture Overview

The model is built on CLIP (Contrastive Language-Image Pre-training) and extends it with multi-modal fusion for affordance understanding:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Input                                    â”‚
â”‚   RGB Image (3Ã—HÃ—W) + Depth Map (1Ã—HÃ—W) + Text Description      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLIP Backbone (RN50)                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚   â”‚ Visual Encoderâ”‚           â”‚ Text Encoder  â”‚                 â”‚
â”‚   â”‚ (ModifiedResNet)          â”‚ (Transformer) â”‚                 â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚           â”‚                           â”‚                          â”‚
â”‚     v2,v3,v4 features          word_features, state             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                           â”‚
            â–¼                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚    Depth Encoder      â”‚               â”‚
â”‚  (DepthEncoder)       â”‚               â”‚
â”‚  feat_8, feat_16      â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
            â”‚                           â”‚
            â–¼                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Multi-Modal FPN (Feature Pyramid Network)           â”‚
â”‚        Fuses visual features (v2+depth_8, v3+depth_16, v4)       â”‚
â”‚                     with text state                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Transformer Decoder (Cross-Attention)               â”‚
â”‚         Visual-Language fusion with positional encoding          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Output Heads                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Projector_Mult â”‚  â”‚  MotionVAE/MLP â”‚  â”‚  TrajectoryMLP     â”‚ â”‚
â”‚  â”‚  (mask + point) â”‚  â”‚  (axis + type) â”‚  â”‚  (3D trajectory)   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Outputs                                  â”‚
â”‚   â€¢ Segmentation Mask (H/4 Ã— W/4)                               â”‚
â”‚   â€¢ Interaction Point Heatmap (H/4 Ã— W/4)                       â”‚
â”‚   â€¢ Soft-argmax Coordinates (x, y) âˆˆ [0,1]                      â”‚
â”‚   â€¢ Motion Direction (3D unit vector)                           â”‚
â”‚   â€¢ Motion Type (translation=0, rotation=1)                     â”‚
â”‚   â€¢ 3D Trajectory (20 points Ã— 3 coords)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
SegAffordance/
â”œâ”€â”€ model/                          # Neural network components
â”‚   â”œâ”€â”€ __init__.py                 # Model builder function
â”‚   â”œâ”€â”€ segmenter.py                # Main CRIS model
â”‚   â”œâ”€â”€ clip.py                     # CLIP backbone (ViT/ResNet)
â”‚   â””â”€â”€ layers.py                   # Custom layers (FPN, Projector, VAE, etc.)
â”‚
â”œâ”€â”€ datasets/                       # Dataset implementations
â”‚   â”œâ”€â”€ opdreal.py                  # OPDReal dataset (real-world)
â”‚   â”œâ”€â”€ opdreal_datamodule.py       # PyTorch Lightning DataModule for OPDReal
â”‚   â”œâ”€â”€ scenefun3d.py               # SceneFun3D dataset (indoor scenes)
â”‚   â”œâ”€â”€ scenefun3d_datamodule.py    # PyTorch Lightning DataModule for SF3D
â”‚   â””â”€â”€ OPDReal/                    # OPDReal utilities and data loaders
â”‚
â”œâ”€â”€ config/                         # Configuration files
â”‚   â”œâ”€â”€ opd_train.py                # Dataclass definitions for configs
â”‚   â”œâ”€â”€ opd_train.yaml              # Base model config
â”‚   â”œâ”€â”€ opdreal_train.yaml          # OPDReal training overrides
â”‚   â”œâ”€â”€ opdmulti_train.yaml         # OPDMulti training overrides
â”‚   â”œâ”€â”€ sf3d_train.yaml             # SceneFun3D training config
â”‚   â”œâ”€â”€ *_test.yaml                 # Test configurations
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ train_OPDReal_better.py         # Training script for OPDReal
â”œâ”€â”€ train_OPDMulti_better.py        # Training script for OPDMulti (finetune)
â”œâ”€â”€ train_SF3D_better.py            # Training script for SceneFun3D
â”‚
â”œâ”€â”€ tests/                          # Test/evaluation scripts
â”‚   â”œâ”€â”€ test_OPDReal_better.py
â”‚   â”œâ”€â”€ test_OPDMulti_better.py
â”‚   â””â”€â”€ test_SF3D_better.py
â”‚
â”œâ”€â”€ utils/                          # Utility functions
â”‚   â”œâ”€â”€ tools.py                    # Loss functions, visualization
â”‚   â”œâ”€â”€ dataset.py                  # Tokenization utilities
â”‚   â””â”€â”€ simple_tokenizer.py         # CLIP tokenizer
â”‚
â”œâ”€â”€ pretrain/                       # Pretrained models
â”‚   â””â”€â”€ RN50.pt                     # CLIP ResNet-50 weights
â”‚
â”œâ”€â”€ train.sh                        # Training launch script
â”œâ”€â”€ slurm.sh                        # SLURM job submission script
â””â”€â”€ requirements.txt                # Python dependencies
```

## ğŸš€ Installation

### Requirements

- Python 3.8+
- PyTorch 1.12+
- CUDA 11.3+

### Setup

```bash
# Clone the repository
git clone <repository-url>
cd SegAffordance

# Create conda environment (recommended)
conda create -n segaffordance python=3.10
conda activate segaffordance

# Install PyTorch (adjust for your CUDA version)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Install dependencies
pip install -r requirements.txt

# Install additional dependencies
pip install pytorch-lightning h5py
```

### Download Pretrained CLIP Weights

Place the CLIP ResNet-50 weights in `pretrain/RN50.pt`. You can download from [OpenAI CLIP](https://github.com/openai/CLIP).

## ğŸ“Š Datasets

### OPDReal (Real-World Articulated Objects)

Real-world dataset with articulated objects (doors, drawers, cabinets, etc.).

**Data structure:**
```
MotionDataset_h5_real/
â”œâ”€â”€ train.h5                    # Training images (HDF5)
â”œâ”€â”€ valid.h5                    # Validation images
â”œâ”€â”€ test.h5                     # Test images
â”œâ”€â”€ depth.h5                    # Depth maps (all splits)
â”œâ”€â”€ annotations_bwdf/           # Filtered annotations
â”‚   â”œâ”€â”€ MotionNet_train.json
â”‚   â”œâ”€â”€ MotionNet_valid.json
â”‚   â””â”€â”€ MotionNet_test.json
â””â”€â”€ real-attr.json              # Object attributes for evaluation
```

### OPDMulti (Synthetic Multi-Object)

Synthetic dataset with multiple articulated objects per scene.

**Data structure:**
```
MotionDataset_h5/
â”œâ”€â”€ train.h5
â”œâ”€â”€ valid.h5  
â”œâ”€â”€ test.h5
â”œâ”€â”€ depth.h5
â””â”€â”€ annotations_bwdf/
    â””â”€â”€ ...
```

### SceneFun3D (Indoor Scenes with Trajectories)

Indoor scene dataset with interaction trajectories.

**Data structure:**
```
sf3d_processed/
â”œâ”€â”€ data.lmdb                   # LMDB database with all data
â”œâ”€â”€ images/                     # RGB images
â””â”€â”€ depth/                      # Depth images (16-bit PNG, mm)
```

## ğŸ¯ Training

### SceneFun3D Training

The standard training workflow copies the LMDB dataset to shared memory for faster I/O:

```bash
# Using train.sh
./train.sh

# Or manually:
cp -r /path/to/sf3d_processed/data.lmdb /dev/shm/data.lmdb
python train_SF3D_better.py fit --config config/sf3d_train.yaml
```

### OPDReal Training

```bash
python train_OPDReal_better.py fit \
    --config config/opd_train.yaml \
    --config config/opdreal_train.yaml
```

### OPDMulti Training (Fine-tuning from OPDReal)

```bash
python train_OPDMulti_better.py fit \
    --config config/opd_train.yaml \
    --config config/opdmulti_train.yaml
```

The OPDMulti config automatically:
- Loads pretrained weights from OPDReal checkpoint
- Freezes backbone, depth encoder, and neck
- Only trains decoder and prediction heads

### SLURM Job Submission

```bash
sbatch slurm.sh
```

## ğŸ“ˆ Testing & Evaluation

### OPDReal Testing

```bash
python train_OPDReal_better.py test \
    --config config/opd_train.yaml \
    --config config/opdreal_train.yaml \
    --config config/opdreal_test.yaml \
    --ckpt_path /path/to/checkpoint.ckpt
```

### OPDMulti Testing

```bash
python train_OPDMulti_better.py test \
    --config config/opd_train.yaml \
    --config config/opdmulti_train.yaml \
    --config config/opdmulti_test.yaml \
    --ckpt_path /path/to/checkpoint.ckpt
```

### SceneFun3D Testing

```bash
python train_SF3D_better.py test \
    --config config/sf3d_train.yaml \
    --config config/sf3d_test.yaml \
    --ckpt_path /path/to/checkpoint.ckpt
```

### Evaluation Metrics

| Metric | Description |
|--------|-------------|
| **Mean IoU** | Intersection over Union for segmentation masks |
| **P_Det** | Detection rate (IoU > threshold) |
| **P_Det+M** | Detection + correct motion type |
| **P_Det+MA** | Detection + motion type + axis direction |
| **P_Det+MAO** | Detection + motion type + axis + origin (for rotation) |
| **ERR_ADir** | Mean axis direction error (degrees) |
| **Mean Point Error** | L2 error for interaction point prediction |
| **Mean Origin Error** | 3D origin error (normalized, for rotational motions) |

## âš™ï¸ Configuration

### Model Parameters (`ModelParams`)

| Parameter | Description | Default |
|-----------|-------------|---------|
| `clip_pretrain` | Path to CLIP weights | `pretrain/RN50.pt` |
| `word_len` | Text token length | 77 |
| `use_depth` | Enable depth fusion | `true` |
| `use_cvae` | Use CVAE for motion (vs MLP) | `true` |
| `fpn_in` | FPN input channels | `[512, 1024, 1024]` |
| `fpn_out` | FPN output channels | `[256, 512, 1024]` |
| `num_layers` | Transformer decoder layers | 3 |
| `num_head` | Attention heads | 8 |
| `dim_ffn` | FFN dimension | 1024 |
| `dropout` | Dropout rate | 0.2 |
| `vae_latent_dim` | VAE latent dimension | 32 |
| `vae_hidden_dim` | VAE hidden dimension | 256 |

### Loss Parameters (`LossParams`)

| Parameter | Description | Default |
|-----------|-------------|---------|
| `mask_weight` | Mask loss weight | 0.5 |
| `point_map_weight` | Point heatmap loss weight | 1.0 |
| `coord_weight` | Coordinate regression weight | 0.5 |
| `vae_weight` | Motion VAE loss weight | 0.5 |
| `motion_type_weight` | Motion type classification weight | 0.5 |
| `trajectory_weight` | Trajectory prediction weight | 0.5 |
| `geometric_weight` | Geometric consistency weight | 0.5 |
| `bce_weight` | BCE weight in DiceBCE | 0.5 |
| `dice_weight` | Dice weight in DiceBCE | 0.5 |
| `point_sigma` | Gaussian heatmap sigma | 8.0 |
| `vae_beta` | KL divergence weight | 0.01 |

### Test Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| `test_iou_threshold` | IoU threshold for matching | 0.5 |
| `test_motion_threshold_deg` | Axis error threshold (degrees) | 10.0 |
| `test_pred_threshold` | Mask binarization threshold | 0.5 |
| `test_match_metric` | Matching metric (`mask` or `bbox`) | `mask` |
| `test_visualize_debug` | Enable debug visualizations | `false` |

## ğŸ”¬ Model Components

### CRIS Model (`model/segmenter.py`)

Main model class that combines all components:

```python
class CRIS(nn.Module):
    def __init__(self, model_params):
        # CLIP backbone for vision and text encoding
        self.backbone = build_model(clip_state_dict, word_len)
        
        # Optional depth encoder
        self.depth_encoder = DepthEncoder(...)
        
        # Multi-modal FPN
        self.neck = FPN(...)
        
        # Transformer decoder for cross-attention
        self.decoder = TransformerDecoder(...)
        
        # Output projector (mask + point)
        self.proj = Projector_Mult(...)
        
        # Motion prediction (CVAE or MLP)
        self.motion_vae = MotionVAE(...)  # or self.motion_mlp
        
        # Trajectory prediction
        self.trajectory_predictor = TrajectoryMLP(...)
```

### Key Layers (`model/layers.py`)

- **`DepthEncoder`**: Encodes depth maps to multi-scale features
- **`FPN`**: Feature Pyramid Network for multi-scale visual-text fusion
- **`TransformerDecoder`**: Cross-attention between visual and text features
- **`Projector_Mult`**: Dynamic kernel convolution for multi-output prediction
- **`MotionVAE`**: Conditional VAE for motion vector prediction
- **`MotionMLP`**: Deterministic MLP alternative to VAE
- **`TrajectoryMLP`**: Predicts 3D trajectory points

### Loss Functions (`utils/tools.py`)

- **`DiceBCELoss`**: Combined Dice + BCE loss for segmentation
- **`MotionVAELoss`**: Reconstruction + KL divergence for motion VAE
- **Geometric Consistency Loss**: Ensures trajectory-motion vector consistency

## ğŸ¨ Visualization

Training visualizations are logged to Weights & Biases (wandb) and include:

- Predicted vs GT segmentation masks
- Interaction point heatmaps
- Motion direction arrows (3D projected)
- Trajectory points (GT: blue, Pred: cyan)
- Motion type annotations

Debug visualizations during testing save images to disk showing:
- Point prediction heatmap overlay
- Mask prediction overlay
- Ground truth annotations

## ğŸ“ Training Tips

1. **Data Loading Speed**: For SceneFun3D, copy LMDB to `/dev/shm` for faster I/O
2. **Transfer Learning**: Train on OPDReal first, then finetune on OPDMulti
3. **Depth Usage**: Depth improves performance on OPDReal; can be disabled with `use_depth: false`
4. **CVAE vs MLP**: CVAE captures multi-modal motion distributions; MLP is simpler
5. **Batch Size**: Use larger batches (128-256) with mixed precision (`precision: 16`)

## ğŸ”— References

- [CLIP: Learning Transferable Visual Models](https://arxiv.org/abs/2103.00020)
- [CRIS: CLIP-Driven Referring Image Segmentation](https://arxiv.org/abs/2111.15174)
- [OPD Dataset](https://3dlg-hcvc.github.io/OPD/)
- [SceneFun3D](https://scenefun3d.github.io/)

## ğŸ“„ License

[Specify your license here]

## ğŸ‘¥ Authors

[Your name/team here]
