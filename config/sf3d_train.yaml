seed_everything: 42

model:
  model_params:
    clip_pretrain: "pretrain/RN50.pt" # Assumes a valid CLIP model is at this path
    word_len: 77                     # Standard CLIP context length
    use_depth: false                 # SF3D does not use depth
    use_cvae: true                   # Toggle CVAE motion head (default true)
    depth_feat_channels: [128, 256]
    fpn_in: [512, 1024, 1024]      # Input channels for FPN from backbone.
    fpn_out: [256, 512, 1024]        # Output channels for FPN layers.
    # Decoder (Transformer)
    num_layers: 3                    # Number of layers in Transformer decoder
    num_head: 8                      # Number of attention heads
    dim_ffn: 1024                    # Dimension of feed-forward network in Transformer
    dropout: 0.2                     # Dropout rate in Transformer
    intermediate: False              # Whether the Transformer decoder returns intermediate layer outputs
    # Projector
    proj_dropout: 0.25
    # VAE for Motion Prediction
    vae_latent_dim: 32
    vae_hidden_dim: 256
    # motion type
    num_motion_types: 2

  loss_params:
    # Loss functions
    bce_weight: 0.5             # Weight for BCE loss in DiceBCELoss
    dice_weight: 0.5            # Weight for Dice loss in DiceBCELoss
    # loss weights
    mask_weight: 0.5           # Add this (previously implicit 1.0)
    point_map_weight: 1.0 
    coord_weight: 0.5          # Weight for the L1 coordinate regression loss
    vae_weight: 0.5
    motion_type_weight: 0.5
    # loss parameters
    point_sigma: 8.0            # Sigma for Gaussian heatmap generation (in pixels on the feature map)
    vae_beta: 0.01

  optimizer_params:
    lr: 0.00001
    weight_decay: 0.0001
    scheduler_milestones: [25, 27]
    scheduler_gamma: 0.1

  config:
    log_image_interval_steps: 100
    input_size: [256, 256]           # Image input size (height, width)
    enable_wandb: True
    val_vis_samples: 12
    manual_seed: 42

trainer:
  max_epochs: 30
  accelerator: "gpu"
  devices: 1
  precision: 16
  logger:
    class_path: pytorch_lightning.loggers.WandbLogger
    init_args:
      name: "SF3D_V13"
      project: "SceneFun3D"
      save_dir: "/cluster/scratch/andrye/wandb_logs/SF3D_V13"
      log_model: false
  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        dirpath: "/cluster/project/cvg/students/andrye/experiments/SF3D_V13"
        monitor: "val/loss_total"
        mode: "min"
        save_top_k: 10
        filename: "best-{epoch}-{val/loss_total:.4f}"
  log_every_n_steps: 10


data:
  train_data_dir: "/cluster/project/cvg/students/andrye/sf3d"
  val_split_ratio: 0.1
  input_size: [256, 256]
  batch_size_train: 128
  batch_size_val: 128
  num_workers_train: 16
  num_workers_val: 16
  manual_seed: 42
